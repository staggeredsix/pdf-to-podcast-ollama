"""
Monologue flow module for converting PDFs to podcast monologues.

This module handles the workflow for generating single-speaker podcast content from PDF documents.
It includes functionality for summarizing PDFs, generating outlines, and creating monologue scripts.
"""

from shared.api_types import JobStatus, TranscriptionRequest  # Job status tracking and request types
from shared.podcast_types import Conversation  # Podcast conversation data structures
from shared.pdf_types import PDFMetadata  # PDF document metadata and content
from shared.llmmanager import LLMManager  # LLM interaction management
from shared.job import JobStatusManager  # Background job status tracking
from typing import List, Dict  # Type hints
import ujson as json  # Fast JSON processing
import logging  # Logging utilities
from shared.prompt_tracker import PromptTracker  # Tracks prompts sent to LLM
from monologue_prompts import FinancialSummaryPrompts  # Prompt templates
from langchain_core.messages import AIMessage  # LLM message type
import asyncio  # Async functionality


async def monologue_summarize_pdf(
    pdf_metadata: PDFMetadata, llm_manager: LLMManager, prompt_tracker: PromptTracker
) -> AIMessage:
    """
    Summarize a single PDF document using the LLM.

    Args:
        pdf_metadata (PDFMetadata): Metadata and content of the PDF to summarize
        llm_manager (LLMManager): Manager for LLM interactions
        prompt_tracker (PromptTracker): Tracks prompts and responses

    Returns:
        AIMessage: The LLM's summary response

    The function uses a template to generate a summary prompt and tracks both the
    prompt and response for monitoring purposes.
    """
    template = FinancialSummaryPrompts.get_template("monologue_summary_prompt")
    prompt = template.render(text=pdf_metadata.markdown)

    summary_response: AIMessage = await llm_manager.query_async(
        "reasoning",
        [{"role": "user", "content": prompt}],
        f"summarize_{pdf_metadata.filename}",
    )
    prompt_tracker.track(
        f"summarize_{pdf_metadata.filename}",
        prompt,
        llm_manager.model_configs["reasoning"].name,
    )
    return summary_response


async def monologue_summarize_pdfs(
    pdfs: List[PDFMetadata],
    job_id: str,
    llm_manager: LLMManager,
    prompt_tracker: PromptTracker,
    job_manager: JobStatusManager,
    logger: logging.Logger,
) -> List[PDFMetadata]:
    """
    Summarize multiple PDFs in the request.

    Args:
        pdfs (List[PDFMetadata]): List of PDFs to summarize
        job_id (str): ID for tracking job progress
        llm_manager (LLMManager): Manager for LLM interactions
        prompt_tracker (PromptTracker): Tracks prompts and responses
        job_manager (JobStatusManager): Manages job status updates
        logger (logging.Logger): Logger for tracking progress

    Returns:
        List[PDFMetadata]: The input PDFs with summaries added

    Uses asyncio.gather to process multiple PDFs concurrently and updates
    job status throughout the process.
    """
    job_manager.update_status(
        job_id, JobStatus.PROCESSING, f"Summarizing {len(pdfs)} PDFs"
    )

    summaries: List[AIMessage] = await asyncio.gather(
        *[monologue_summarize_pdf(pdf, llm_manager, prompt_tracker) for pdf in pdfs]
    )

    for pdf, summary in zip(pdfs, summaries):
        pdf.summary = summary.content
        prompt_tracker.update_result(f"summarize_{pdf.filename}", pdf.summary)
        logger.info(f"Successfully summarized {pdf.filename}")

    return pdfs


async def monologue_generate_raw_outline(
    summarized_pdfs: List[PDFMetadata],
    request: TranscriptionRequest,
    llm_manager: LLMManager,
    prompt_tracker: PromptTracker,
    job_id: str,
    job_manager: JobStatusManager,
) -> str:
    """
    Generate an initial outline from the summarized PDFs.

    Args:
        summarized_pdfs (List[PDFMetadata]): PDFs with their summaries
        request (TranscriptionRequest): Original transcription request
        llm_manager (LLMManager): Manager for LLM interactions
        prompt_tracker (PromptTracker): Tracks prompts and responses
        job_id (str): ID for tracking job progress
        job_manager (JobStatusManager): Manages job status updates

    Returns:
        str: Raw outline text generated by the LLM

    Combines PDF summaries and any focus instructions to generate a structured
    outline for the monologue.
    """
    job_manager.update_status(
        job_id, JobStatus.PROCESSING, "Generating initial outline"
    )

    # Format documents as a string list for consistency with podcast flow
    documents = [f"Document: {pdf.filename}\n{pdf.summary}" for pdf in summarized_pdfs]

    template = FinancialSummaryPrompts.get_template(
        "monologue_multi_doc_synthesis_prompt"
    )
    prompt = template.render(
        focus_instructions=request.guide if request.guide else None,
        documents="\n\n".join(documents),
    )

    raw_outline: AIMessage = await llm_manager.query_async(
        "reasoning",
        [{"role": "user", "content": prompt}],
        "raw_outline",
    )

    prompt_tracker.track(
        "raw_outline",
        prompt,
        llm_manager.model_configs["reasoning"].name,
        raw_outline.content,
    )

    return raw_outline.content


async def monologue_generate_monologue(
    raw_outline: str,
    request: TranscriptionRequest,
    llm_manager: LLMManager,
    prompt_tracker: PromptTracker,
    job_id: str,
    job_manager: JobStatusManager,
) -> str:
    """
    Generate a complete monologue transcript from the outline.

    Args:
        raw_outline (str): Generated outline to expand into monologue
        request (TranscriptionRequest): Original transcription request
        llm_manager (LLMManager): Manager for LLM interactions
        prompt_tracker (PromptTracker): Tracks prompts and responses
        job_id (str): ID for tracking job progress
        job_manager (JobStatusManager): Manages job status updates

    Returns:
        str: Complete monologue transcript

    Expands the outline into a natural-sounding monologue, incorporating
    the speaker's name and any focus areas specified in the request.
    """
    job_manager.update_status(
        job_id, JobStatus.PROCESSING, "Creating monologue transcript"
    )

    template = FinancialSummaryPrompts.get_template("monologue_transcript_prompt")
    prompt = template.render(
        raw_outline=raw_outline,
        documents=request.pdf_metadata,
        focus=request.guide
        if request.guide
        else "key financial metrics and performance indicators",
        speaker_1_name=request.speaker_1_name,
    )

    monologue: AIMessage = await llm_manager.query_async(
        "reasoning",
        [{"role": "user", "content": prompt}],
        "create_monologue",
    )

    prompt_tracker.track(
        "create_monologue",
        prompt,
        llm_manager.model_configs["reasoning"].name,
        monologue.content,
    )

    return monologue.content


async def monologue_create_final_conversation(
    monologue: str,
    request: TranscriptionRequest,
    llm_manager: LLMManager,
    prompt_tracker: PromptTracker,
    job_id: str,
    job_manager: JobStatusManager,
) -> Conversation:
    """
    Convert the monologue into a structured Conversation format.

    Args:
        monologue (str): Generated monologue transcript
        request (TranscriptionRequest): Original transcription request
        llm_manager (LLMManager): Manager for LLM interactions
        prompt_tracker (PromptTracker): Tracks prompts and responses
        job_id (str): ID for tracking job progress
        job_manager (JobStatusManager): Manages job status updates

    Returns:
        Conversation: Structured conversation object

    Formats the monologue into a JSON structure that matches the Conversation
    schema, handling proper text escaping and validation.
    """
    job_manager.update_status(
        job_id, JobStatus.PROCESSING, "Formatting final conversation"
    )

    schema = Conversation.model_json_schema()
    template = FinancialSummaryPrompts.get_template("monologue_dialogue_prompt")
    prompt = template.render(
        speaker_1_name=request.speaker_1_name,
        text=monologue,
        schema=json.dumps(schema, indent=2),
    )

    conversation_json: Dict = await llm_manager.query_async(
        "json",
        [{"role": "user", "content": prompt}],
        "create_final_conversation",
        json_schema=schema,
    )

    # Ensure all strings are unescaped
    if "dialogues" in conversation_json:
        for entry in conversation_json["dialogues"]:
            if "text" in entry:
                entry["text"] = unescape_unicode_string(entry["text"])

    prompt_tracker.track(
        "create_final_conversation",
        prompt,
        llm_manager.model_configs["json"].name,
        json.dumps(conversation_json),
    )

    return Conversation.model_validate(conversation_json)


def unescape_unicode_string(s: str) -> str:
    """
    Convert escaped Unicode sequences to actual Unicode characters.

    Args:
        s (str): String potentially containing escaped Unicode sequences

    Returns:
        str: String with Unicode sequences properly unescaped

    This handles both raw strings (with extra backslashes) and regular strings,
    ensuring proper display of special characters in the final output.
    """
    return s.encode("utf-8").decode("unicode-escape")
